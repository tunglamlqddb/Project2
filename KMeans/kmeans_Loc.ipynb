{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Kn_G8RTK-Nbc9diqI9qxxLxQKKFjGng_","authorship_tag":"ABX9TyPoDe1/Q7z/5eepJ6qhIxzU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"sByFkwuRwztN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random\n","from collections import defaultdict\n","import sys\n","import time\n","\n","\n","class Point:\n","    def __init__(self, label, doc_id, tfidf):\n","        def normalize():\n","            # compute norm2 square\n","            ans = 0.0\n","            for x in tfidf.values():\n","                ans += x**2\n","            ans=ans**0.5\n","            for i in tfidf:\n","                tfidf[i] = tfidf[i]/ans\n","\n","            self.tfidf = tfidf\n","\n","        self.label = label\n","        self.doc_id = doc_id\n","        normalize()\n","\n","\n","class Cluster:\n","    def __init__(self):\n","        self.centroid = None\n","        self.points = []\n","        self.centroid_l2_square = 0.0\n","\n","    def set_centroid(self, new_centroid):\n","        self.centroid = new_centroid\n","\n","    def add_point(self, point):\n","        self.points.append(point)\n","\n","    def reset_points(self):\n","        self.points = []\n","\n","def Load_data(pathin):\n","    def tfidf(doc):\n","        tf_idf = defaultdict(int)\n","        fea = doc.split()\n","        for i in fea:\n","            tmp = i.split(':')\n","            tf_idf[int(tmp[0])] = float(tmp[1])\n","        return tf_idf\n","\n","    with open(pathin, 'r') as f:\n","        d_lines = f.read().splitlines()\n","    data = []\n","    for d in d_lines:\n","        fea = d.split('<<>>')\n","        label = fea[0]\n","        doc_id = fea[1]\n","        data.append(Point(label, doc_id,tfidf(fea[2])))\n","    return data\n","\n","X_train = Load_data(\"/content/drive/My Drive/Project2/Datasets/train_tfidf_df=3\")\n","X_test = Load_data(\"/content/drive/My Drive/Project2/Datasets/test_tfidf\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDtRyw67xF63","colab_type":"code","colab":{}},"source":["\n","class Kmeans:\n","    def __init__(self):\n","        self.list_clusters = []\n","        self.k_cluster = 0\n","        self.n_doc = 0\n","        self.new_clusters = []\n","        self.max_simirality = 0.0\n","        self.it=0\n","        '''\n","        with open(\"C:\\\\Users\\\\nql\\\\Desktop\\\\20192\\\\project2\\\\textpreprocessing\\\\words_idf\", 'r') as f:\n","            self.dim = len(f.read().splitlines())\n","        '''\n","        self.dim=20167\n","\n","    def dist_between_x_y(self, x, y):\n","        '''\n","        x is a point;\n","        y is a vector\n","        '''\n","        # compute dot product\n","        ans = 0.0\n","        for pos in x.tfidf:\n","            ans += x.tfidf[pos]*y[pos]\n","        # return distance\n","        return (2-2*ans)\n","\n","    def init_centroid(self, X):\n","        rd = random.sample(range(self.n_doc), self.k_cluster)\n","        for i in rd:\n","            centroid = np.zeros(self.dim)\n","            for pos in X[i].tfidf:\n","                centroid[pos] = X[i].tfidf[pos]\n","            tmp = Cluster()\n","            tmp.set_centroid(centroid)\n","            self.list_clusters.append(tmp)\n","\n","    \n","    def init_centroid_v1(self, X):\n","        def add_centroid(idx):\n","            centroid = np.zeros(self.dim)\n","            for pos in X[idx].tfidf:\n","                centroid[pos] = X[idx].tfidf[pos]\n","            new_cluster = Cluster()\n","            new_cluster.set_centroid(centroid)\n","            self.list_clusters.append(new_cluster)\n","\n","        # khoi tao 1 centroid ban dau\n","        add_centroid(random.randint(1, self.n_doc))\n","        # mang luu tru khoang cach nho nhat cua diem x voi cac centroid khoi tao\n","        dist_min_between_x_centroid=np.array([sys.maxsize]*self.n_doc,dtype=float)\n","        # chon cac centroid khac sao cho khoang cach den cac centroid da tao lon nhat\n","        for k in range(self.k_cluster-1):\n","            for i in range(self.n_doc):\n","                d=self.dist_between_x_y(X[i],self.list_clusters[-1].centroid)\n","                dist_min_between_x_centroid[i]=min(dist_min_between_x_centroid[i],d)\n","            idx_new_centroid=np.argmax(dist_min_between_x_centroid)\n","            add_centroid(idx_new_centroid)\n","\n","\n","    def update_centroid(self):\n","        max_simirality = -1\n","        for clr in self.new_clusters:\n","            new_centroid = np.zeros(self.dim)\n","            for pnt in clr.points:\n","                for pos in pnt.tfidf:\n","                    new_centroid[pos] += pnt.tfidf[pos]\n","            new_centroid = new_centroid/len(clr.points)\n","            #new_centroid=new_centroid/np.linalg.norm(new_centroid)\n","            clr.set_centroid(new_centroid)\n","        \n","\n","    def asign_cluster(self, X):\n","        self.new_clusters.clear()\n","        for i in range(self.k_cluster):\n","            tmp=Cluster()\n","            self.new_clusters.append(tmp)\n","\n","        for x in X:\n","            dis_min = sys.maxsize\n","            asign_clr = -1\n","            for i in range(self.k_cluster):\n","                dis = self.dist_between_x_y(x,self.list_clusters[i].centroid)\n","                if(dis <dis_min):\n","                    asign_clr = i\n","                    dis_min = dis\n","            self.new_clusters[asign_clr].add_point(x)\n","\n","    \n","    def check_stop(self, label_criteria):\n","        count_labels_change = self.n_doc\n","        for i in range(self.k_cluster):\n","            labels_unchange =[ label for label in self.list_clusters[i].points\n","                              if label in self.new_clusters[i].points ]\n","            count_labels_change -= len(labels_unchange)\n","        if(count_labels_change <label_criteria):\n","            return True\n","\n","        return False\n","    \n","    \n","    def fit(self, X,k_cluster):\n","        def backup():\n","            self.list_clusters.clear()\n","            self.list_clusters= [ clr for clr in self.new_clusters]\n","\n","        def run_batch(X_batch):\n","            for i in range(100):\n","                self.asign_cluster(X_batch)\n","                self.update_centroid()\n","                if(self.check_stop(10)):\n","                    self.it=i\n","                    backup()\n","                    break\n","                backup()\n","\n","        \n","        self.n_doc = len(X)\n","        self.k_cluster = k_cluster\n","        self.init_centroid_v1(X)\n","\n","        # chay tim centroid tren tap du lieu nho\n","        #rd =random.sample(range(self.n_doc),2000)\n","        #run_batch(X[rd])\n","\n","        # khi tim duoc centroid tren tap dl nho roi, tim tren tap train\n","        run_batch(X)\n","\n","    def predict(self, X_test):\n","        self.asign_cluster(X_test)\n","        n_test = len(X_test)\n","        count_predict_true = 0\n","        for clr in self.new_clusters:\n","            pre = [0]*self.k_cluster\n","            for x in clr.points:\n","                pre[int(x.label)] += 1\n","            count_predict_true += np.max(pre)\n","        return str(count_predict_true)+'/'+str(n_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpoyJwKPdgyr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"9ecccf87-0718-4437-d610-24f6233deb8c","executionInfo":{"status":"ok","timestamp":1588592071631,"user_tz":-420,"elapsed":2760107,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["for i in range(10):\n","  t=time.time()\n","  model= Kmeans()\n","  model.fit(X_train,20)\n","  print(\"time train =\",time.time()-t,'; iter =',model.it,end='; ')\n","  ans_train=model.predict(X_train)\n","  print('pre train =', ans_train,end=';')\n","  ans_test=model.predict(X_test)\n","  print('pre test= ',ans_test)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["time train = 271.4844000339508 ; iter = 24; pre train = 4958/11314;pre test=  3233/7532\n","time train = 192.11541604995728 ; iter = 17; pre train = 6005/11314;pre test=  3935/7532\n","time train = 225.0636281967163 ; iter = 20; pre train = 5527/11314;pre test=  3575/7532\n","time train = 265.3886787891388 ; iter = 24; pre train = 5132/11314;pre test=  3297/7532\n","time train = 224.64220190048218 ; iter = 20; pre train = 5856/11314;pre test=  3849/7532\n","time train = 186.50940895080566 ; iter = 16; pre train = 5098/11314;pre test=  3327/7532\n","time train = 405.68641996383667 ; iter = 37; pre train = 6489/11314;pre test=  4199/7532\n","time train = 300.1251771450043 ; iter = 27; pre train = 5661/11314;pre test=  3911/7532\n","time train = 278.77742314338684 ; iter = 25; pre train = 5843/11314;pre test=  3735/7532\n","time train = 245.94400119781494 ; iter = 22; pre train = 6026/11314;pre test=  3890/7532\n"],"name":"stdout"}]}]}