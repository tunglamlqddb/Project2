{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random \n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    def __init__ (self, K, centroid_difference, max_iters):\n",
    "        self.K = K\n",
    "        self.centroid_difference = centroid_difference\n",
    "        self.max_iters = max_iters\n",
    "    \n",
    "    def get_data(self, path_tfidfs, path_idfs, isTrained):\n",
    "        \n",
    "        # sparse to dense\n",
    "        def sparse_to_dense(sparsed_doc, vocab_size):\n",
    "            densed_doc = np.zeros(vocab_size)\n",
    "            split_sparsed_doc = sparsed_doc.split()\n",
    "            for item in split_sparsed_doc:\n",
    "                idx = int(item.split(':')[0])\n",
    "                tfidf = float(item.split(':')[1])\n",
    "                densed_doc[idx] = tfidf\n",
    "            return densed_doc\n",
    "        \n",
    "        # get vocab_size\n",
    "        with open(path_idfs) as f:\n",
    "            vocab_size = len(f.read().splitlines())\n",
    "            \n",
    "        # get tfidf of each doc and transform from sparse vetorto dense vector:\n",
    "        with open(path_tfidfs) as f:\n",
    "            doc_lines = f.read().splitlines()\n",
    "        if isTrained == True:\n",
    "            self.data = []      # each doc is an item of data\n",
    "        else:\n",
    "            self.data_test = []\n",
    "            \n",
    "        for doc in doc_lines:\n",
    "            features = doc.split('<fff>')\n",
    "            label = int(features[0])\n",
    "            doc_id = int(features[1])\n",
    "            dense_doc = sparse_to_dense(features[2], vocab_size)\n",
    "            \n",
    "            if isTrained == True:\n",
    "                self.data.append((label, dense_doc))    \n",
    "            else:\n",
    "                self.data_test.append((label, dense_doc))\n",
    "                \n",
    "            \n",
    "    def initialize(self):    # k-means++ initialization\n",
    "        self.centroids = {}    # a dict to store K centroids; key of this dict to identify a centroid belong to which cluster\n",
    "        # randomly pick one data point as the first centroid\n",
    "        self.centroids[0] = self.data[random.randrange(0, len(self.data)-1, 1)][1]\n",
    "        idx = 0\n",
    "        d = sys.maxsize\n",
    "        # a list in which each item is the distance between a data point and the nearest previously chosen centroid \n",
    "        dist_points_centroid = [d]*len(self.data)\n",
    "        dist_points_centroid = np.array(dist_points_centroid)\n",
    "        \n",
    "        # loop to find K-1 centroids\n",
    "        for centroid_idx in range(self.K - 1):\n",
    "            for i in range(len(self.data)):\n",
    "                point = self.data[i][1]\n",
    "                # only need to compute distance between the point and the newest centroid\n",
    "                temp_dist = np.sum((self.centroids[idx] - point)**2)\n",
    "                dist_points_centroid[i] = min(dist_points_centroid[i], temp_dist)\n",
    "          \n",
    "            # choose the data point that has the largest distance with its nearest centroid\n",
    "            next_centroid = self.data[np.argmax(dist_points_centroid)][1]\n",
    "            self.centroids[idx+1] = next_centroid\n",
    "            idx += 1\n",
    "                \n",
    "                \n",
    "    def assign_example(self, example):       # func to assign one doc to its cluster\n",
    "        doc = example[1]\n",
    "        dist = sys.maxsize\n",
    "        class_idx = -1\n",
    "        for j in self.centroids:\n",
    "            temp_dist = np.sum((doc - self.centroids[j])**2)\n",
    "            if (dist > temp_dist):\n",
    "                class_idx = j\n",
    "                dist = temp_dist\n",
    "        self.clusters[class_idx].append(example)\n",
    "    \n",
    "    def fit(self, data_set):\n",
    "    \n",
    "        # begin iterations\n",
    "        for i in range(self.max_iters):\n",
    "            \n",
    "            self.clusters = {}\n",
    "            for i in range(self.K):\n",
    "                self.clusters[i] = []    # list of (actual label - doc) belonging to this clusters\n",
    "        \n",
    "            for item in data_set:\n",
    "                self.assign_example(item)\n",
    "            \n",
    "            list_previous_centroids = [list(centroid) for centroid in self.centroids.values()]\n",
    "            \n",
    "            # caculate new centroids\n",
    "            for cluster_idx in self.clusters:\n",
    "                if len(self.clusters[cluster_idx]) > 0:\n",
    "                    doc_list = [item[1] for item in self.clusters[cluster_idx]]\n",
    "                    aver_doc = np.mean(doc_list, axis=0)\n",
    "                    denom = np.sqrt(np.sum(aver_doc ** 2))\n",
    "                   # assert denom > 0\n",
    "                    new_centroid = np.array([numerator / denom for numerator in aver_doc])\n",
    "                    self.centroids[cluster_idx] = new_centroid\n",
    "\n",
    "            isOptimal = False\n",
    "            \n",
    "            list_current_centroids = [list(centroid) for centroid in self.centroids.values()]\n",
    "    \n",
    "            # difference between old centroids and new centroids\n",
    "            minus_centroids = [centroid for centroid in list_current_centroids if centroid not in list_previous_centroids]\n",
    "            if len(minus_centroids) <= self.centroid_difference:\n",
    "                isOptimal = True\n",
    "            if isOptimal == True:\n",
    "                break\n",
    "      \n",
    "    def predict(self, data_set):\n",
    "        for i in range(self.K):\n",
    "            self.clusters[i] = []  \n",
    "        for item in data_set:\n",
    "            self.assign_example(item)\n",
    "    \n",
    "    def compute_purity(self, data_set, justFit):\n",
    "        if justFit == False:\n",
    "            self.predict(data_set)    # predict data_set to get its centroids and clusters\n",
    "        \n",
    "        count = 0\n",
    "        for i in self.clusters:\n",
    "            labels_of_cluster = [member[0] for member in self.clusters[i]]\n",
    "            count_max_label = max([labels_of_cluster.count(label) for label in range(20)])\n",
    "            count += count_max_label\n",
    "        return count * 1. / len(data_set)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(km.data_test))\n",
    "#km.data[11313][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = K_Means(20, 10, 100)\n",
    "km.get_data(\"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\Project2\\\\words_tfidfs.txt\", \"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\Project2\\\\words_idfs.txt\", True)\n",
    "km.get_data(\"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\Project2\\\\words_tfidfs_test.txt\", \"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\Project2\\\\words_idfs.txt\", False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  355.6602427959442\n",
      "purity of training set:  0.39437864592540217\n",
      "-------------------\n",
      "training time:  515.011504650116\n",
      "purity of training set:  0.4102881385893583\n",
      "-------------------\n",
      "training time:  626.3926403522491\n",
      "purity of training set:  0.3580519710093689\n",
      "-------------------\n",
      "training time:  613.0481243133545\n",
      "purity of training set:  0.3975605444581934\n",
      "-------------------\n",
      "training time:  520.6776068210602\n",
      "purity of training set:  0.42301573272052323\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "best_model = km\n",
    "tmp_training_purity = 0\n",
    "best_training_purity  = 0\n",
    "\n",
    "for i in range(5):\n",
    "    t1 = time.time()\n",
    "    km.initialize()\n",
    "    km.fit(km.data)\n",
    "    t2 = time.time()\n",
    "    print(\"training time: \", t2-t1)\n",
    "    tmp_training_purity = km.compute_purity(km.data, True)\n",
    "    if (tmp_training_purity > best_training_purity):\n",
    "        best_model = km\n",
    "    print(\"purity of training set: \", tmp_training_purity)\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestt model\n",
      "purity of tesing set:  0.3996282527881041\n"
     ]
    }
   ],
   "source": [
    "print(\"bestt model\")\n",
    "print(\"purity of tesing set: \", best_model.compute_purity(best_model.data_test, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
