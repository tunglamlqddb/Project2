{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer       # split words and punctuations\n",
    "import string\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess(stringDoc):  # stringDoc is a list of lines from one doc\n",
    "    \n",
    "    def splitWord(words):       \n",
    "        new_punctuations = (string.punctuation)\n",
    "        table = str.maketrans('', '', new_punctuations)     # remove punctuations\n",
    "        tk = WordPunctTokenizer() \n",
    "        new_word = tk.tokenize(words)\n",
    "        new_word = [word.translate(table) for word in new_word]            # delete those punctuations\n",
    "        return new_word \n",
    "    \n",
    "    \n",
    "    def preprocess(words):      # input is an array of words (array of strings)\n",
    "\n",
    "        table1 = str.maketrans('', '', '\\t')        # remove tabs\n",
    "        words = [word.translate(table1) for word in words]    # translate func works with string    \n",
    "\n",
    "        #there are enters at several lines, or at the end of a line when reading line by line => remove them\n",
    "        table3 = str.maketrans('', '', '\\n')\n",
    "        words = [word.translate(table3) for word in words]\n",
    "\n",
    "        res = []\n",
    "        for word in words:\n",
    "            tem = splitWord(word)\n",
    "            for i in tem:\n",
    "                res.append(i)\n",
    "        words = res\n",
    "\n",
    "        # remove spaces (maybe after translation, --- becomes space)\n",
    "        words = [word for word in words if word]\n",
    "\n",
    "        # remove numbers\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "        # remove number from word (ex: 3DDD)\n",
    "        remove_digits = str.maketrans('', '', digits) \n",
    "        words = [word.translate(remove_digits) for word in words]\n",
    "\n",
    "        # after removing numbers, there might be single characters or 2-letter-characters, remove them\n",
    "        words = [word for word in words if len(word) > 2]\n",
    "\n",
    "        # lowercase\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        # final step: remove left blank spaces \n",
    "        words = [word for word in words if word]\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def remove_metadata(listOfLines):\n",
    "        for i in range(len(listOfLines)):\n",
    "            if (listOfLines[i] == '\\n'):\n",
    "                break\n",
    "        newList = listOfLines[i+1:]\n",
    "        return newList\n",
    "    \n",
    "    def remove_stopwords(words):          # input is an array of words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        number = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    "        '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n",
    "        for i in number:\n",
    "            stop_words.add(i)\n",
    "        new_words = [word for word in words if word not in stop_words]\n",
    "        return new_words\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "    def sentence_tokenize(line):          # token a single sentence in form of a string, return an array of words\n",
    "        ps = PorterStemmer()\n",
    "        words = line.strip().split()      # strip() removes leading and trailing spaces, split() removes inbetween spaces, split into list of words \n",
    "        words = preprocess(words)         # after process, words are in lowercase => can check with stopwords\n",
    "        words = remove_stopwords(words)\n",
    "        words = [ps.stem(word) for word in words]\n",
    "        return words\n",
    "    \n",
    "    def tokenize(listOfLines):           # token a whole document in a form of list of lines (strings)\n",
    "        # firstly, remove metadata\n",
    "        #newList = remove_metadata(listOfLines)             \n",
    "\n",
    "        wordsOfDoc = []     # an array to store words of one document\n",
    "        for line in listOfLines:\n",
    "            words = sentence_tokenize(line)\n",
    "            if (len(words) > 0):              # remove enter-character-line\n",
    "                wordsOfDoc.append(words)\n",
    "        return wordsOfDoc\n",
    "    \n",
    "    def flatten(list):\n",
    "        res = []\n",
    "        for i in list:\n",
    "            for j in i: \n",
    "                res.append(j)\n",
    "        return res\n",
    "    \n",
    "    result = flatten(tokenize(stringDoc))    # transform a document to a vector\n",
    "    \n",
    "    return result      # an array of cleaned words from one doc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, listClasses):\n",
    "        self.classes = listClasses\n",
    "    \n",
    "    def addToBow(self, example, cat_idx):     # example is a list of words from one doc\n",
    "        for word in example:\n",
    "            self.bow_dicts[cat_idx][word] += 1\n",
    "    \n",
    "    def train(self, dataset, labels):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.bow_dicts = np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        if not isinstance(self.labels, np.ndarray): \n",
    "            self.labels = np.array(self.labels)\n",
    "            \n",
    "        # create bow_dicts\n",
    "        for cat_idx, cat in enumerate(self.classes):\n",
    "            idx_of_cat_in_labels = np.where(self.labels == cat)[0]   # positions of doc belonging to cat\n",
    "            cat_dataset = []\n",
    "            for idx in idx_of_cat_in_labels:\n",
    "                cat_dataset.append(dataset[idx])\n",
    "            for example in cat_dataset:\n",
    "                self.addToBow(example, cat_idx)\n",
    "                \n",
    "        \n",
    "        prob_classes = np.zeros(self.classes.shape[0])    # pre prob P(c_i)\n",
    "        cat_words_count = np.empty(self.classes.shape[0])  # total #of words of each category\n",
    "        denoms = np.empty(self.classes.shape[0])\n",
    "        \n",
    "        for cat_idx, cat in enumerate(self.classes):\n",
    "           \n",
    "            # caculate P(c_i)\n",
    "            prob_classes[cat_idx] = np.sum(self.labels == cat) / float(self.labels.shape[0])\n",
    "            \n",
    "            # caculate total counts of all the words of each class\n",
    "            cat_words_count[cat_idx] = sum(list(self.bow_dicts[cat_idx].values()))\n",
    "        \n",
    "        #caculate denominator of P(word_j|c_i) for each class\n",
    "        denoms = np.array([cat_words_count[cat_idx] + len(trimmed_uniqueWords) for cat_idx, cat in enumerate(self.classes)])\n",
    "            \n",
    "        self.prob_classes = prob_classes\n",
    "        self.denoms = denoms\n",
    "        \n",
    "        \n",
    "    def getExampleProb(self, example):     # example is a list of words of one test doc\n",
    "        likelihood_prob = np.zeros(self.classes.shape[0])   # store log(P(t_j|c_i))\n",
    "        post_prob = np.zeros(self.classes.shape[0])     # store post prob of each class\n",
    "        for cat_idx, cat in enumerate(self.classes):\n",
    "            for word in example:\n",
    "                word_counts_in_cat = self.bow_dicts[cat_idx].get(word, 0) + 1\n",
    "                word_prob = word_counts_in_cat / float(self.denoms[cat_idx])\n",
    "                likelihood_prob[cat_idx] += np.log(word_prob)\n",
    "            post_prob[cat_idx] = likelihood_prob[cat_idx] + np.log(self.prob_classes[cat_idx])\n",
    "        \n",
    "        return post_prob\n",
    "    \n",
    "    def test(self, testset):\n",
    "        predictions = []\n",
    "        for example in testset:\n",
    "            post_prob = self.getExampleProb(example)\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "train_path = \"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\20news-bydate\\\\20news-bydate-train\"\n",
    "test_path = \"D:\\\\movedFromC\\\\123\\\\20192\\\\PRJ2\\\\20news-bydate\\\\20news-bydate-test\"\n",
    "\n",
    "folders = [f for f in listdir(train_path)]     # labels\n",
    "\n",
    "singleBOW = []\n",
    "train_label = []\n",
    "singleBOWTest = []\n",
    "test_label = []\n",
    "# read train data\n",
    "for i in range(len(folders)):\n",
    "    files = listdir(join(train_path, folders[i]))\n",
    "    for j in range(len(files)):\n",
    "        contentFile = open(join(join(train_path, folders[i]), files[j]), \"r\")\n",
    "        singleBOW.append(preprocess(contentFile.readlines()))\n",
    "        train_label.append(i)\n",
    "        \n",
    "# read test data\n",
    "for i in range(len(folders)):\n",
    "    files = listdir(join(test_path, folders[i]))\n",
    "    for j in range(len(files)):\n",
    "        contentFile = open(join(join(test_path, folders[i]), files[j]), \"r\")\n",
    "        singleBOWTest.append(preprocess(contentFile.readlines()))\n",
    "        test_label.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = []                       # a dictionary of whole docs\n",
    "for i in range(len(singleBOW)):\n",
    "    uniqueWords = set(uniqueWords).union(set(singleBOW[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76336"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "for tem in singleBOW:\n",
    "    for word in tem:\n",
    "        dfs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 10\n",
    "\n",
    "trimmed_uniqueWords = [word for word in uniqueWords if dfs[word] > df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12493"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trimmed_uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_singleBOW = []\n",
    "for tem in singleBOW:\n",
    "    trimmed_tem = [word for word in tem if dfs[word] > df]\n",
    "    trimmed_singleBOW.append(trimmed_tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trimmed_singleBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "nb = NaiveBayes(np.unique(train_label))\n",
    "t = time.time()\n",
    "nb.train(trimmed_singleBOW, train_label)\n",
    "train_time = time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "predictions = nb.test(singleBOWTest)\n",
    "\n",
    "accuracy = np.sum(predictions==test_label) / float(len(test_label))\n",
    "\n",
    "accuracy * 100\n",
    "\n",
    "test_time = time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.87254381306425"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.804133415222168"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.52934098243713"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join. for fit_transform\n",
    "preprocessedData = []\n",
    "for i in range(len(singleBOW)):\n",
    "    preprocessedData.append(\" \".join(singleBOW[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedDataTest = []\n",
    "for i in range(len(singleBOWTest)):\n",
    "    preprocessedDataTest.append(\" \".join(singleBOWTest[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(min_df = 11)\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(preprocessedData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tf_vectorizer.transform(preprocessedDataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "t_lib = time.time()\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, train_label)\n",
    "\n",
    "train_time_lib = time.time() - t_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07823395729064941"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lib = time.time()\n",
    "y_pred = naive_bayes_classifier.predict(X_test_tf)\n",
    "\n",
    "accuracy = np.sum(y_pred==test_label) / float(len(test_label))\n",
    "\n",
    "accuracy * 100\n",
    "\n",
    "test_time_lib = time.time() - t_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.6558682952735"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03663825988769531"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_time_lib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
